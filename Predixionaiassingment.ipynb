{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Engineer Assignment: Hindi Conversation Analysis\n",
    "\n",
    "Author: Mukul Chavan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Llama-cpp for model loading and for using its create_chat_completion function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T05:59:28.582296Z",
     "iopub.status.busy": "2024-07-17T05:59:28.581947Z",
     "iopub.status.idle": "2024-07-17T06:00:51.760361Z",
     "shell.execute_reply": "2024-07-17T06:00:51.759416Z",
     "shell.execute_reply.started": "2024-07-17T05:59:28.582264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/llama-cpp-wheels/llamacpp\n",
      "Processing /kaggle/input/llama-cpp-wheels/llamacpp/llama_cpp_python-0.2.25.tar.gz\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.2.25) (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.2.25) (1.24.3)\n",
      "Processing /kaggle/input/llama-cpp-wheels/llamacpp/diskcache-5.6.3-py3-none-any.whl (from llama-cpp-python==0.2.25)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.25-cp310-cp310-manylinux_2_35_x86_64.whl size=8210153 sha256=5892ab795d70248bbde863f698bbb0ef5a1b5450c0f7c27b1879b9f870f29ec3\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/6d/fa/bb7e80f98dd8f8d64c7ac7e906bbf6bf21ea20de0dfca6039c\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.25\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.25 --no-index --find-links=file:///kaggle/input/llama-cpp-wheels/llamacpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Download a quantized LLM model named MetaMath Cybertron Starling directly from hugging face repository. This LLM model is trained on 7 Billion parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:29:30.007227Z",
     "iopub.status.busy": "2024-07-17T06:29:30.006419Z",
     "iopub.status.idle": "2024-07-17T06:29:30.138686Z",
     "shell.execute_reply": "2024-07-17T06:29:30.137797Z",
     "shell.execute_reply.started": "2024-07-17T06:29:30.007194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.cache/huggingface/hub/models--TheBloke--MetaMath-Cybertron-Starling-GGUF/snapshots/cc6631af2b1d9350082ef4004d574c89ebc74278/metamath-cybertron-starling.Q8_0.gguf'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id='TheBloke/MetaMath-Cybertron-Starling-GGUF', \n",
    "                  filename='metamath-cybertron-starling.Q8_0.gguf')         #Here I am using 8-bit quantize version of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:29:50.810583Z",
     "iopub.status.busy": "2024-07-17T06:29:50.810203Z",
     "iopub.status.idle": "2024-07-17T06:29:53.539861Z",
     "shell.execute_reply": "2024-07-17T06:29:53.538803Z",
     "shell.execute_reply.started": "2024-07-17T06:29:50.810548Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--MetaMath-Cybertron-Starling-GGUF/snapshots/cc6631af2b1d9350082ef4004d574c89ebc74278/metamath-cybertron-starling.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = q-bert_metamath-cybertron-starling\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = q-bert_metamath-cybertron-starling\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "warning: failed to mlock 7695859712-byte buffer (after previously locking 0 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MLOCK ('ulimit -l' as root).\n",
      "llm_load_tensors: system memory used  =  132.92 MiB\n",
      "llm_load_tensors: VRAM used           = 7205.83 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "warning: failed to mlock 140001280-byte buffer (after previously locking 0 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MLOCK ('ulimit -l' as root).\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2000\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  250.00 MiB, K (f16):  125.00 MiB, V (f16):  125.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 5.58 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 2.39 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7208.22 MiB (model: 7205.83 MiB, context: 2.39 MiB)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "model_path1 = '/root/.cache/huggingface/hub/models--TheBloke--MetaMath-Cybertron-Starling-GGUF/snapshots/cc6631af2b1d9350082ef4004d574c89ebc74278/metamath-cybertron-starling.Q8_0.gguf'\n",
    "n_gpu_layers = -1\n",
    "n_batch = 8 \n",
    "\n",
    "llm = Llama(\n",
    "    model_path = model_path1,\n",
    "    chat_format = \"llama-2\",          #chat format of the LLM model(MetaMath-Cybertron-Starling)\n",
    "    n_gpu_layers = -1,\n",
    "    n_batch = n_batch,\n",
    "    use_mlock = True,\n",
    "    f16_kv = True,\n",
    "    n_ctx = 2000,\n",
    "    main_gpu = 0,\n",
    "    verbose = True,\n",
    "    seed = 1330\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:29:59.006214Z",
     "iopub.status.busy": "2024-07-17T06:29:59.005362Z",
     "iopub.status.idle": "2024-07-17T06:29:59.012323Z",
     "shell.execute_reply": "2024-07-17T06:29:59.011282Z",
     "shell.execute_reply.started": "2024-07-17T06:29:59.006177Z"
    }
   },
   "outputs": [],
   "source": [
    "hindi_convo=\"\"\"\n",
    "Recovery Agent (RA): नमस्ते श्री कुमार, मैं एक्स वाई जेड फाइनेंस से बोल रहा हूं। आपके लोन के बारे में बात करनी थी।\n",
    "\n",
    "Borrower (B): हां, बोलिए। क्या बात है?\n",
    "\n",
    "RA: सर, आपका पिछले महीने का EMI अभी तक नहीं आया है। क्या कोई समस्या है?\n",
    "\n",
    "B: हां, थोड़ी दिक्कत है। मेरी नौकरी चली गई है और मैं नया काम ढूंढ रहा हूं।\n",
    "\n",
    "RA: ओह, यह तो बुरा हुआ। लेकिन सर, आपको समझना होगा कि लोन का भुगतान समय पर करना बहुत जरूरी है।\n",
    "\n",
    "B: मैं समझता हूं, लेकिन अभी मेरे पास पैसे नहीं हैं। क्या कुछ समय मिल सकता है?\n",
    "\n",
    "RA: हम समझते हैं आपकी स्थिति। क्या आप अगले हफ्ते तक कुछ भुगतान कर सकते हैं?\n",
    "\n",
    "B: मैं कोशिश करूंगा, लेकिन पूरा EMI नहीं दे पाऊंगा। क्या आधा भुगतान चलेगा?\n",
    "\n",
    "RA: ठीक है, आधा भुगतान अगले हफ्ते तक कर दीजिए। बाकी का क्या प्लान है आपका?\n",
    "\n",
    "B: मुझे उम्मीद है कि अगले महीने तक मुझे नया काम मिल जाएगा। तब मैं बाकी बकाया चुका दूंगा।\n",
    "\n",
    "RA: ठीक है। तो हम ऐसा करते हैं - आप अगले हफ्ते तक आधा EMI जमा कर दीजिए, और अगले महीने के 15 तारीख तक बाकी का भुगतान कर दीजिए। क्या यह आपको स्वीकार है?\n",
    "\n",
    "B: हां, यह ठीक रहेगा। मैं इस प्लान का पालन करने की पूरी कोशिश करूंगा।\n",
    "\n",
    "RA: बहुत अच्छा। मैं आपको एक SMS भेज रहा हूं जिसमें भुगतान की डिटेल्स होंगी। कृपया इसका पालन करें और समय पर भुगतान करें।\n",
    "\n",
    "B: ठीक है, धन्यवाद आपके समझने के लिए।\n",
    "\n",
    "RA: आपका स्वागत है। अगर कोई और सवाल हो तो मुझे बताइएगा। अलविदा।\n",
    "\n",
    "B: अलविदा।\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For system prompting I have used chain-of-thought approach of prompting. Chain-of-thought prompt helps the LLM model to understand easily with the step by step instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:29:59.489415Z",
     "iopub.status.busy": "2024-07-17T06:29:59.489027Z",
     "iopub.status.idle": "2024-07-17T06:29:59.495200Z",
     "shell.execute_reply": "2024-07-17T06:29:59.494140Z",
     "shell.execute_reply.started": "2024-07-17T06:29:59.489377Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\\\n",
    "You are a helpful AI assistant that examines the conversations of Finance company.\\\n",
    "You will get Hindi conversations and you have to give analysis of that conversation keeping in mind the cultural context.\\\n",
    "Follow the step by step instructions to give the analysis.\n",
    "\n",
    "Step 1: Examine the whole conversation and summarize it concisely within 100 words. \\\n",
    "Give the Heading as \"Summary:-\".\n",
    "\n",
    "Step 2: Find the key actions performed or next steps identified from the conversation. \\\n",
    "Limit those key points upto 5 bullet points.\\\n",
    "Give the Heading as \"Key actions:-\" and write the analysis on the next line.\n",
    "\n",
    "Step 3: Write the sentiment anaylsis from the overall tone of each participant and how it changes throughout the conversation. \\\n",
    "Give specific examples from the conversation to support the analysis of sentiments to proove. \\\n",
    "Give the Heading as \"Sentiment Anaylsis:-\" and write the analysis on the next line and limit the whole anaylsis upto 100 words.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\\\n",
    "Hindi conversation:{hindi_convo}\n",
    "The Anaylsis of the conversation: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every LLM model has its own chat template. Hence with the help of Llama-Cpp's create_chat_completion we can use any model without worring to change the chat template of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:29:59.988097Z",
     "iopub.status.busy": "2024-07-17T06:29:59.987722Z",
     "iopub.status.idle": "2024-07-17T06:32:08.217597Z",
     "shell.execute_reply": "2024-07-17T06:32:08.216609Z",
     "shell.execute_reply.started": "2024-07-17T06:29:59.988065Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     276.89 ms\n",
      "llama_print_timings:      sample time =     129.38 ms /   254 runs   (    0.51 ms per token,  1963.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   90922.67 ms /  1528 tokens (   59.50 ms per token,    16.81 tokens per second)\n",
      "llama_print_timings:        eval time =   35491.14 ms /   253 runs   (  140.28 ms per token,     7.13 tokens per second)\n",
      "llama_print_timings:       total time =  128221.31 ms\n"
     ]
    }
   ],
   "source": [
    "ans = llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {'role':'system', 'content':system_prompt},\n",
    "        {'role':'user', 'content':user_prompt}\n",
    "    ],\n",
    "    max_tokens = 1500,\n",
    "    temperature = 0.01,   #temperature is set to minimum so that LLM will give more accurate answer.\n",
    "    stop =\"[/INST]\"       #stop words to break the answer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaylsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:32:08.219404Z",
     "iopub.status.busy": "2024-07-17T06:32:08.219091Z",
     "iopub.status.idle": "2024-07-17T06:32:08.224265Z",
     "shell.execute_reply": "2024-07-17T06:32:08.223412Z",
     "shell.execute_reply.started": "2024-07-17T06:32:08.219376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summary:- In this conversation, a recovery agent from X2J Finance talks to a borrower about his overdue EMI. The borrower explains that he is unemployed and looking for new work. The recovery agent suggests a payment plan where the borrower pays half of the EMI by next week and the remaining amount by the 15th of the following month. The borrower agrees to this plan and both parties express their gratitude.\n",
      "\n",
      "Key actions:-\n",
      "- Recovery agent explains the situation of overdue EMI.\n",
      "- Borrower shares his unemployment status and plans to find new work.\n",
      "- Recovery agent suggests a payment plan for the borrower.\n",
      "- Borrower agrees to the payment plan.\n",
      "\n",
      "Sentiment Anaylsis:-\n",
      "At the beginning, the tone is serious as the recovery agent informs the borrower about the overdue EMI. The borrower's tone becomes more emotional when he shares his unemployment status and financial difficulties. However, both parties show understanding and cooperation in finding a solution. The tone becomes positive when they agree on a payment plan. Overall, the conversation has a mix of seriousness, empathy, and positivity.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ans['choices'][0]['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 159451933,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
